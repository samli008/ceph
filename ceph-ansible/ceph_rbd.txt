#view osd info
ceph osd tree
ceph osd df 
ceph df
ceph -s
ceph mon stat

# cd /etc/ceph
ceph-deploy disk zap c04:vdb
ceph-deploy osd create c04:vdb

#create rbd pool
ceph osd pool create rbd 128 128
ceph osd pool set rbd size 2 
ceph osd pool application enable rbd rbd 
rbd create disk1 -s 100G --image-feature layering
rbd info disk1
rbd ls -l
rbd showmapped
rbd map disk1 
rbd unmap disk1
rbd rm disk1

#export osd crushmap
ceph osd getcrushmap -o map.bin
crushtool -d map.bin -o map.txt

#osd crushmap default rule type host ; one ceph node modify default rule type osd before osd create; edit map.txt with vi;

#import osd crushmap
crushtool -c map.txt -o map.bin.new
ceph osd setcrushmap -i map.bin.new

# Total PGs = ((Total_number_of_OSD * 100) / max_replication_count) / pool_count

ceph osd pool get rbd pg_num
ceph osd pool set rbd pg_num 256
ceph osd pool get rbd pgp_num
ceph osd pool set rbd pgp_num 256
